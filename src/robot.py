# –∑–∞–ø—É—Å–∫ —Ä–æ–±–æ—Ç–∞ –¥–ª—è –º–∞—Å–æ–≤–æ—ó –∞—Ä—Ö—ñ–≤–∞—Ü—ñ—ó:
# docker compose exec kdv-api python3 -m src.robot

import requests
import time
import logging
import sys
import os
from .config import KDV_API_TOKEN


# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [ROBOT] %(levelname)s: %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOG_DIR, "robot_batch.log")),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger("Robot")

API_BASE = "http://localhost:5000/kdv/api"
HEADERS = {"X-KDV-TOKEN": KDV_API_TOKEN}
POLL_INTERVAL = 3  # —Å–µ–∫—É–Ω–¥–∏ –ø–µ—Ä–µ—Ä–≤–∏ –º—ñ–∂ –æ–ø–∏—Ç—É–≤–∞–Ω–Ω—è–º —Å—Ç–∞—Ç—É—Å—É
BATCH_DELAY = 5    # —Å–µ–∫—É–Ω–¥–∏ –ø–µ—Ä–µ—Ä–≤–∏ –º—ñ–∂ –∫–Ω–∏–≥–∞–º–∏ (—â–æ–± –Ω–µ "–ø–æ–∫–ª–∞—Å—Ç–∏" DSpace)

def parse_candidates(filename):
    """
    –ü–∞—Ä—Å–∏—Ç—å —Ñ–∞–π–ª candidates.txt, –ø—ñ–¥—Ç—Ä–∏–º—É—é—á–∏ –¥—ñ–∞–ø–∞–∑–æ–Ω–∏ —Ç–∞ —Å–ø–∏—Å–∫–∏.
    –ü—Ä–∏–∫–ª–∞–¥–∏ —Ä—è–¥–∫—ñ–≤ —É —Ñ–∞–π–ª—ñ:
      14
      20, 21, 25
      100-110
      300-305, 400
    """
    if not os.path.exists(filename):
        logger.error(f"File {filename} not found!")
        return []

    unique_ids = set()

    with open(filename, 'r') as f:
        for line in f:
            # –í–∏–¥–∞–ª—è—î–º–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ —Ç–∞ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
            line = line.split('#')[0].strip()
            if not line: continue

            # –†–æ–∑–±–∏–≤–∞—î–º–æ –ø–æ –∫–æ–º—ñ (—è–∫—â–æ —î –ø–µ—Ä–µ–ª—ñ–∫ –≤ –æ–¥–Ω–æ–º—É —Ä—è–¥–∫—É)
            parts = line.split(',')
            
            for part in parts:
                part = part.strip()
                if not part: continue

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –¥—ñ–∞–ø–∞–∑–æ–Ω (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ "14-30")
                if '-' in part:
                    try:
                        start_s, end_s = part.split('-')
                        start = int(start_s)
                        end = int(end_s)
                        
                        # –ó–∞—Ö–∏—Å—Ç –≤—ñ–¥ "30-14" (–º—ñ–Ω—è—î–º–æ –º—ñ—Å—Ü—è–º–∏)
                        if start > end: start, end = end, start
                        
                        # –î–æ–¥–∞—î–º–æ –≤–µ—Å—å –¥—ñ–∞–ø–∞–∑–æ–Ω (–≤–∫–ª—é—á–Ω–æ –∑ –æ—Å—Ç–∞–Ω–Ω—ñ–º)
                        for i in range(start, end + 1):
                            unique_ids.add(i)
                    except ValueError:
                        logger.error(f"‚ö†Ô∏è Invalid range format ignored: '{part}'")
                
                # –ó–≤–∏—á–∞–π–Ω–µ —á–∏—Å–ª–æ
                elif part.isdigit():
                    unique_ids.add(int(part))
                else:
                    logger.warning(f"‚ö†Ô∏è Invalid ID format ignored: '{part}'")

    # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Ä—è–¥–∫—ñ–≤
    sorted_ids = sorted(list(unique_ids))
    return [str(i) for i in sorted_ids]

def process_single_biblio(biblionumber):
    """
    –í–∏–∫–æ–Ω—É—î –ø–æ–≤–Ω–∏–π —Ü–∏–∫–ª –∞—Ä—Ö—ñ–≤–∞—Ü—ñ—ó –¥–ª—è –æ–¥–Ω—ñ—î—ó –∫–Ω–∏–≥–∏:
    POST (Start) -> Polling (Wait) -> Result
    """
    logger.info(f"‚ñ∂Ô∏è Processing Biblio #{biblionumber}...")

    # 1. –Ü–Ω—ñ—Ü—ñ–∞—Ü—ñ—è (POST)
    try:
        resp = requests.post(f"{API_BASE}/integrate/{biblionumber}", headers=HEADERS)
        
        # –û–±—Ä–æ–±–∫–∞ —Å—Ç–∞—Ç—É—Å—ñ–≤ HTTP
        if resp.status_code == 409:
            # 409 Conflict: –≤–∂–µ –æ–±—Ä–æ–±–ª—è—î—Ç—å—Å—è –∞–±–æ –∑–∞–±–ª–æ–∫–æ–≤–∞–Ω–æ
            logger.warning(f"‚ö†Ô∏è #{biblionumber} SKIPPED: Already processed/locked.")
            return "SKIPPED"
        
        if resp.status_code == 400 or resp.status_code == 404:
             logger.error(f"‚ùå #{biblionumber} CLIENT ERROR: {resp.json().get('message')}")
             return "ERROR_CLIENT"

        if resp.status_code != 202:
            logger.error(f"‚ùå #{biblionumber} POST Failed ({resp.status_code}): {resp.text}")
            return "ERROR_POST"
            
        data = resp.json()
        task_id = data.get('task_id')
        if not task_id:
            logger.error(f"‚ùå #{biblionumber} No task_id returned!")
            return "ERROR_NO_TASK"
            
        logger.info(f"   Task started: {task_id}. Waiting...")

    except Exception as e:
        logger.error(f"‚ùå #{biblionumber} Connection Error: {e}")
        return "ERROR_CONN"

    # 2. –û—á—ñ–∫—É–≤–∞–Ω–Ω—è (Polling)
    waited = 0
    max_wait = 900 # 15 —Ö–≤–∏–ª–∏–Ω –º–∞–∫—Å–∏–º—É–º (–¥–ª—è –¥—É–∂–µ –≤–µ–ª–∏–∫–∏—Ö —Ñ–∞–π–ª—ñ–≤)
    
    while waited < max_wait:
        time.sleep(POLL_INTERVAL)
        waited += POLL_INTERVAL
        
        try:
            status_resp = requests.get(f"{API_BASE}/status/{task_id}", headers=HEADERS)
            
            if status_resp.status_code == 404:
                 # –Ü–Ω–∫–æ–ª–∏ –±—É–≤–∞—î race condition, —Å–ø—Ä–æ–±—É—î–º–æ —â–µ —Ä–∞–∑
                 continue
            
            if status_resp.status_code != 200:
                logger.warning(f"   Status check failed ({status_resp.status_code}). Retrying...")
                continue
                
            s_data = status_resp.json()
            status = s_data.get('status')
            
            if status == 'success':
                res = s_data.get('result', {})
                handle = res.get('handle')
                uuid = res.get('uuid')
                special_status = res.get('status') # linked_existing?
                
                if special_status == 'linked_existing':
                    logger.info(f"üîÑ #{biblionumber} LINKED (Duplicate): {handle}")
                    return "LINKED"
                else:
                    logger.info(f"‚úÖ #{biblionumber} SUCCESS! Handle: {handle}")
                    return "SUCCESS"
            
            elif status == 'error':
                err_msg = s_data.get('error')
                logger.error(f"‚ùå #{biblionumber} FAILED: {err_msg}")
                return "FAILED"
            
            # –Ø–∫—â–æ processing/queued - —á–µ–∫–∞—î–º–æ –¥–∞–ª—ñ
            
        except Exception as e:
            logger.warning(f"   Polling exception: {e}")

    logger.error(f"‚ùå #{biblionumber} TIMEOUT (waited {max_wait}s)")
    return "TIMEOUT"

def run_batch(filename="candidates.txt"):
    ids = parse_candidates(filename)
    
    if not ids:
        logger.warning("No candidates found via parse logic. Exiting.")
        return

    logger.info("="*40)
    logger.info(f"üìã BATCH STARTED. Candidates: {len(ids)}")
    logger.info(f"   List: {', '.join(ids[:10])} ...") # –ü–æ–∫–∞–∑–∞—Ç–∏ –ø–µ—Ä—à—ñ 10
    logger.info("="*40)
    
    stats = {
        "SUCCESS": 0, 
        "FAILED": 0, 
        "SKIPPED": 0, 
        "LINKED": 0, 
        "TIMEOUT": 0,
        "ERROR_CLIENT": 0,
        "ERROR_CONN": 0
    }
    
    for i, bib_id in enumerate(ids):
        logger.info(f"--- Item {i+1}/{len(ids)} ---")
        result = process_single_biblio(bib_id)
        
        # –°–ø—Ä–æ—â–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∑–≤—ñ—Ç—É
        key = result if result in stats else "FAILED"
        stats[key] = stats.get(key, 0) + 1
        
        # –ü–∞—É–∑–∞ –º—ñ–∂ –∫–Ω–∏–≥–∞–º–∏, —â–æ–± DSpace –≤—Å—Ç–∏–≥ "–≤–∏–¥–∏—Ö–Ω—É—Ç–∏" (—ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è Solr)
        if i < len(ids) - 1:
            time.sleep(BATCH_DELAY)

    logger.info("="*40)
    logger.info(f"üèÅ BATCH COMPLETED.")
    logger.info(f"üìä Stats: {stats}")
    logger.info(f"üìù See full details in robot_batch.log")

if __name__ == "__main__":
    # –î–ª—è –∑–∞–ø—É—Å–∫—É: docker compose exec kdv-api python3 -m src.robot
    run_batch("candidates.txt")